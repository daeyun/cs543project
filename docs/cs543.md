Extracting informative graphics from packaging designs

CS 543

Daeyun Shin

May 12, 2014


Introduction

The goal of the project is to detect unique, identifiable, or informative
graphics from product packaging designs and labels -- for example, logos, symbols, and illustrations.
By design principles, such graphics tend to have unique colors and
shapes that set them apart from the rest of the image. One of the practical uses of this project is to output images that can be used
to further identify and categorize the product like humans would.

In this report, we discuss a data-driven approach that learns objectness cues and general shape features in a sliding image window to detect
identifiable parts of packaging designs and mimic human fixations. We also briefly mention the implementation and pipeline of the learning system, from
data annotation to utilization of commodity cloud computing services such as AWS EC2's high-CPU instances.


Background

In problems that cannot easily be solved analytically, such as extracting arbitarily useful data or mimicing human behavior,
it is often hard to find an analytical evaluation method. For example, the biggest challenge this project faces in
detection of graphic marks such as logos, as much as it may seem clearly defined, is the fact that there is no clear definition
of what a logo is. The difference between a logo and a highly salient text or symbol is essentially down to
the recognition value based on prior knowledge. This ambiguity makes unsupervised bottom-up approaches less useful and strategies
incorporating high-level, top-down models based on what we already know about illustrations and logo-like graphics designed to appeal to consumers.

Such features are often based on design principles such as color contrasts, sizes, locations, edge density, and 
a category of illustrations
that people are more likely to notice even without prior knowledge (mostly for evolutionary reasons) such as human faces,
animals, and certain combinations of shapes and colors. Prior studies on visual
saliency, human fixations, and objectness cues have introduced improvements in efficient models for
detecting regions of images that humans are likely to remember and consider more important. A study by Alexe et al. discusses a
sliding-window based approach that learns objectness features based on a combination of five models:
1. Multi-scale saliency based on salient pixel density, 2. color contrast from surrounding areas (as we already know higher contrast
increases objectness), 3. edge density, 4. superpixel segmentation, 5. and location and size.


Implementation

In this project, due to limited time and resources, we will try to efficiently mimic state-of-the-art techniques
using a two-step sliding-window-based detection system extracting general shape features from HOG descriptors and objectness cues from
chi-squared LAB color distances between three levels of surrounding regions, and normalized location and size 
in a pre-labeled rectangular container boundary. We learn those features using two classifiers.

We first use a high-recall, low precision binary classifier (which is generally computationally less expensive) to
narrow down probable image windows, and then use a secondary high-precision, low-recall classifier to further classify the remaining windows.
This detection pipeline was inspired by an approach used in Google Street View (Frome et al.) for large scale, general object
detection and automated privacy protection.


Data Collection and Annotation

An essential part of many data-driven solutions, especially when detecting objects without clear definition like in this project,
is sufficient amount of human-labeled data. As a part of this project, we implemented a modern, web-based image annotation software
written in CoffeeScript (compiled to JavaScript) and HTML5 that is easily extensible and available. Although this is a one-person project
and all of the data had to be annotated by a single person due to limited resources, implementing a user-friendly (keyboard-shortcut
provided) framework for quickly annotating rectangular regions with multiple annotations was the first step in this project.

%% TODO: include figures with explanations somewhere


Image window detection

The rest of this project, including feature extraction, training, and image window detection is written in Python
using open source libraries such as OpenCV, NumPy, and Scikit-Learn.


Feature extraction

Each square image window (resized to 96 by 96 pixels) is quantized into a vector of 71 normalized floating point numbers in the following categories:

 - 64 HOG descriptor values, computing 16 orientations of Sobel gradient approximations in four cells
 - 3 color contrast dissimilarity values. We first compute 4 by 8 by 8 LAB space histograms of the ROI (image window) in the two
        levels surrounding areas (side length incremented 1.5 times each) and the overall source image. We then compute the chi-squared
        distances using OpenCV's hisgoram functions and take the inverse to normalize in the [0, 1] range.
 - 4 values representing the relative location of the center of the image window, width, and height normalized in the [0, 1] range.

The first attempt was to use a larger HOG descriptor and planning on adding in more features later, the detector's performance was
very slow on a personal computer, so the decision was to reduce the feature space size and vectorizing the code.

While we were experimenting, because of limited project deadline, we decided to run computationally expensive tasks on Amazon EC2's
high-CPU instances. All the code written for this project including the scripts to automatically transfer data, run distributed task
(in fact a naive task dividing strategy based on hash values ranging in the process ID scope and writing the output directly to a remote
filesystem on a master server) will be available online for those who want to experiment with personal projects -- although it was
developed in a very short term and is not suitable for production environment. We tried running the Python feature extraction and training
system on up to 19 high CPU instances. It took less than 10 minutes to train over 200,000 positive and negative feature vectors (mostly near-reduntant image
patches coming from 300 high-resolution photos).


Training

We have experimented with various classifiers in the Scikit-Learn library. K-nearest-neighbors (n=5) and SVM were suitable for
the first-step, high-recall detector (\~0.65 recall). For the high-precision detector, only SVM performed well (0.97 precision and 0.12 recall)
enough to work in our system. The first thought was to use neural networks, but because of the time contraints and Python environment,
we weren't able to experiment much with it although Scikit-Learn's Restricted Boltzmann Machine classifier had a disappointing performance of
0.65 precision and was out-performed by SVM. All training and evaluations were done using 10 fold cross validation.

Our final choice was to use SVM for both detectors. SVM trained with low C (penalty parameter) values was used for the high-recall detector, and
SVM trained with high C and kernel coefficients were suitable for the high-precision detector.


Detection and post processing

After detecting probable image windows using the method outlined above, the detected rectangular areas were groupped using opencv's GroupRectangle
function. The final results for each step are shown in the next section.
